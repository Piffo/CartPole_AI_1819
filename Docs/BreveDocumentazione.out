\BOOKMARK [0][-]{chapter.1}{Introduzione}{}% 1
\BOOKMARK [1][-]{section.1.1}{Cart Pole}{chapter.1}% 2
\BOOKMARK [1][-]{section.1.2}{RL e controllo}{chapter.1}% 3
\BOOKMARK [1][-]{section.1.3}{Cart Pole environments and reward}{chapter.1}% 4
\BOOKMARK [0][-]{chapter.2}{Implementazione}{}% 5
\BOOKMARK [1][-]{section.2.1}{Algoritmo Q learning}{chapter.2}% 6
\BOOKMARK [2][-]{subsection.2.1.1}{Perch\350 Q-learning?}{section.2.1}% 7
\BOOKMARK [2][-]{subsection.2.1.2}{Q-learning off policy control}{section.2.1}% 8
\BOOKMARK [2][-]{subsection.2.1.3}{Scelta dell'azione e Q table}{section.2.1}% 9
\BOOKMARK [2][-]{subsection.2.1.4}{Result}{section.2.1}% 10
\BOOKMARK [1][-]{section.2.2}{Algoritmo DQN}{chapter.2}% 11
\BOOKMARK [2][-]{subsection.2.2.1}{Q network}{section.2.2}% 12
\BOOKMARK [2][-]{subsection.2.2.2}{Replay memory}{section.2.2}% 13
\BOOKMARK [2][-]{subsection.2.2.3}{Result}{section.2.2}% 14
\BOOKMARK [1][-]{section.2.3}{Algoritmo Finite Differences}{chapter.2}% 15
\BOOKMARK [2][-]{subsection.2.3.1}{Approccio Policy gradient estimation}{section.2.3}% 16
\BOOKMARK [2][-]{subsection.2.3.2}{Algoritmo FD}{section.2.3}% 17
\BOOKMARK [2][-]{subsection.2.3.3}{Result}{section.2.3}% 18
