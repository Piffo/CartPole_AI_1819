\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduzione}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Cart Pole}{1}{section.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Cart pole e variabili di stato\relax }}{1}{figure.caption.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:CartPole}{{1.1}{1}{Cart pole e variabili di stato\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}RL e controllo}{1}{section.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Concetto di input-output\relax }}{2}{figure.caption.4}}
\newlabel{fig:ActionBehaviour}{{1.2}{2}{Concetto di input-output\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Rete di retroazione\relax }}{2}{figure.caption.5}}
\newlabel{fig:ControlTheory}{{1.3}{2}{Rete di retroazione\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Modellizzazione \textit  {black - box}\relax }}{3}{figure.caption.6}}
\newlabel{fig:SqueezingOfControlTheory}{{1.4}{3}{Modellizzazione \textit {black - box}\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Sequenza \textit  {State-Action-Reward}\relax }}{3}{figure.caption.7}}
\newlabel{fig:State_Action_Reward}{{1.5}{3}{Sequenza \textit {State-Action-Reward}\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Policy\relax }}{3}{figure.caption.8}}
\newlabel{fig:Policy}{{1.6}{3}{Policy\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Significato di \textit  {imparare} per un agente\relax }}{3}{figure.caption.9}}
\newlabel{fig:PolicyFunction}{{1.7}{3}{Significato di \textit {imparare} per un agente\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Policy update con algoritmi di \textit  {RL}\relax }}{4}{figure.caption.10}}
\newlabel{fig:Policy_update}{{1.8}{4}{Policy update con algoritmi di \textit {RL}\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces RL inteso come \textit  {control theory}\relax }}{4}{figure.caption.11}}
\newlabel{fig:RL_Control}{{1.9}{4}{RL inteso come \textit {control theory}\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Cart Pole environments and reward}{4}{section.1.3}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Implementazione}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Algoritmo Q learning}{7}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Perch\IeC {\`e} Q-learning?}{7}{subsection.2.1.1}}
\newlabel{eq:discounted_return}{{2.1.1}{7}{Perch√® Q-learning?}{subsection.2.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces 4 value functions utilizzate per la policy update\relax }}{8}{figure.caption.12}}
\newlabel{fig:ValueFunctions}{{2.1}{8}{4 value functions utilizzate per la policy update\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Q-learning off policy control}{8}{subsection.2.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Equazione di \textit  {Bellman} utilizzata per andare ad aggiornare i valori della funzione Q\relax }}{8}{figure.caption.13}}
\newlabel{fig:Q_estimation}{{2.2}{8}{Equazione di \textit {Bellman} utilizzata per andare ad aggiornare i valori della funzione Q\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Aggiornamento Q table\relax }}{8}{figure.caption.14}}
\newlabel{fig:Q_update}{{2.3}{8}{Aggiornamento Q table\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Scelta dell'azione e Q table}{8}{subsection.2.1.3}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Q learning per la stima della policy $\pi $\relax }}{9}{algocf.1}}
\newlabel{alg:Q_policy}{{1}{9}{Q-learning off policy control}{algocf.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Metodo per la stima della azione da eseguire\relax }}{9}{figure.caption.16}}
\newlabel{fig:ChooseAction_Bucket}{{2.4}{9}{Metodo per la stima della azione da eseguire\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Possibile struttura della Q-table\relax }}{10}{figure.caption.17}}
\newlabel{fig:Q_table_example}{{2.5}{10}{Possibile struttura della Q-table\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Result}{10}{subsection.2.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Andamento del Q learning\relax }}{10}{figure.caption.18}}
\newlabel{fig:QLearning_result}{{2.6}{10}{Andamento del Q learning\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Andamento del Q learning confrontabile con atri algoritmi implementati\relax }}{11}{figure.caption.19}}
\newlabel{fig:QLearning_poor_result}{{2.7}{11}{Andamento del Q learning confrontabile con atri algoritmi implementati\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Algoritmo DQN}{11}{section.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces DQN network\relax }}{11}{figure.caption.20}}
\newlabel{fig:DQN_network}{{2.8}{11}{DQN network\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Q network}{11}{subsection.2.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Modello della rete neurale: 4 input \textit  {state} e 2 output \textit  {Q-action}\relax }}{12}{figure.caption.21}}
\newlabel{fig:Model_of_DQN_network}{{2.9}{12}{Modello della rete neurale: 4 input \textit {state} e 2 output \textit {Q-action}\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Replay memory}{12}{subsection.2.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Creazione variabile per sequential memory\relax }}{12}{figure.caption.22}}
\newlabel{fig:SeqMem}{{2.10}{12}{Creazione variabile per sequential memory\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Result}{13}{subsection.2.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Reward con DQN network\relax }}{13}{figure.caption.23}}
\newlabel{fig:DQN_reward}{{2.11}{13}{Reward con DQN network\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Algoritmo \textit  {Finite Differences}}{14}{section.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Valutazione della funzione sulla base della Q-values\relax }}{14}{figure.caption.24}}
\newlabel{fig:Q_function_utility}{{2.12}{14}{Valutazione della funzione sulla base della Q-values\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Valutazione della funzione \textit  {non} basandosi sulla Q-values\relax }}{14}{figure.caption.25}}
\newlabel{fig:NO_Q_function_utility}{{2.13}{14}{Valutazione della funzione \textit {non} basandosi sulla Q-values\relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Approccio \textit  {Policy gradient estimation}}{15}{subsection.2.3.1}}
\newlabel{eq:J_function_to_minimize}{{2.3.1}{15}{Approccio \textit {Policy gradient estimation}}{subsection.2.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Algoritmo \textit  {FD}}{16}{subsection.2.3.2}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Finite Differences algorithm\relax }}{16}{algocf.2}}
\newlabel{alg:FD_alg}{{2}{16}{Algoritmo \textit {FD}}{algocf.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Reward \textit  {Finite Differences in Python}\relax }}{16}{figure.caption.27}}
\newlabel{fig:FD_py_alg}{{2.14}{16}{Reward \textit {Finite Differences in Python}\relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsubsection}{RBF network}{16}{section*.28}}
\newlabel{sec:RBF}{{2.3.2}{16}{RBF network}{section*.28}{}}
\@writefile{toc}{\contentsline {subsubsection}{\textit  {Phi} function}{16}{section*.29}}
\@writefile{toc}{\contentsline {subsubsection}{Rollout}{16}{section*.30}}
\newlabel{sec:rollout}{{2.3.2}{16}{Rollout}{section*.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Result}{16}{subsection.2.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces Reward \textit  {Finite Differences}\relax }}{17}{figure.caption.31}}
\newlabel{fig:Reward_FD}{{2.15}{17}{Reward \textit {Finite Differences}\relax }{figure.caption.31}{}}
\bibcite{eco:tesi}{1}
\bibcite{mori:tesi}{2}
