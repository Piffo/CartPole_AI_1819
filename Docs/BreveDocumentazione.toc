\babel@toc {english}{}
\contentsline {chapter}{\numberline {1}Introduzione}{1}{chapter.1}
\contentsline {section}{\numberline {1.1}Cart Pole}{1}{section.1.1}
\contentsline {section}{\numberline {1.2}RL e controllo}{1}{section.1.2}
\contentsline {section}{\numberline {1.3}Cart Pole environments and reward}{4}{section.1.3}
\contentsline {chapter}{\numberline {2}Implementazione}{7}{chapter.2}
\contentsline {section}{\numberline {2.1}Algoritmo Q learning}{7}{section.2.1}
\contentsline {subsection}{\numberline {2.1.1}Perch\IeC {\`e} Q-learning?}{7}{subsection.2.1.1}
\contentsline {subsection}{\numberline {2.1.2}Q-learning off policy control}{8}{subsection.2.1.2}
\contentsline {subsection}{\numberline {2.1.3}Scelta dell'azione e Q table}{8}{subsection.2.1.3}
\contentsline {subsection}{\numberline {2.1.4}Result}{10}{subsection.2.1.4}
\contentsline {section}{\numberline {2.2}Algoritmo DQN}{11}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Q network}{11}{subsection.2.2.1}
\contentsline {subsection}{\numberline {2.2.2}Replay memory}{12}{subsection.2.2.2}
\contentsline {subsection}{\numberline {2.2.3}Result}{13}{subsection.2.2.3}
\contentsline {section}{\numberline {2.3}Algoritmo \textit {Finite Differences}}{14}{section.2.3}
\contentsline {subsection}{\numberline {2.3.1}Approccio \textit {Policy gradient estimation}}{15}{subsection.2.3.1}
\contentsline {subsubsection}{RBF network}{15}{section*.26}
\contentsline {subsubsection}{\textit {Phi} function}{15}{section*.27}
\contentsline {subsubsection}{Rollout}{15}{section*.28}
\contentsline {subsection}{\numberline {2.3.2}Result}{15}{subsection.2.3.2}
